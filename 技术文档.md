# Transformer+feature pyramid

## MNT模块

![image-20250125210355945](C:\Users\Ste'fan\AppData\Roaming\Typora\typora-user-images\image-20250125210355945.png)

1.切分图片成每个小块patches

2.**patch embedding**：根据VIT模型，进行patch embedding,输出向量()

3**.local aggregation：**

​	3.1深度逐层点卷积，输入1x3向量，输出1x3向量(W*H*C)

​	3.2 PositionEmbedding，根据VIT

4**.FeedForward+Sparse Attention+Resnet**

![image-20250226151636063](C:\Users\Ste'fan\AppData\Roaming\Typora\typora-user-images\image-20250226151636063.png)







## 稀疏注意力机制讲解

嗯，用户问的是sparse attention和普通attention在代码上的差异。我需要先回忆一下这两者的区别。普通attention，比如Transformer里的自注意力，计算的是所有位置之间的点积，所以计算复杂度是O(n²)，当序列很长时，这很耗资源。而sparse attention则是通过限制每个token只能关注到其他的一部分token，比如局部窗口或者固定步长的位置，来减少计算量。

那在代码实现上，普通attention的Q@K^T会得到一个N x N的矩阵，而sparse的话可能只计算其中一部分。例如，可能生成一个mask，把不需要的位置设为负无穷，这样softmax之后这些位置的权重就是0。或者是在计算的时候，只取特定的位置进行计算，避免全部计算。

用户提供的普通Attention代码中，forward函数里计算attn = (q @ k.transpose(-2, -1)) * self.scale，这一步得到的就是全连接的注意力矩阵。而sparse的话可能需要在这里做改动，比如引入mask或者限制k和v的选取范围。

比如说，如果是局部窗口的sparse attention，每个query只和附近的几个key做计算。这时候可能需要将输入序列分成块，或者使用滑动窗口来限制每个query的关注范围。或者像Longformer那样使用滑动窗口，每个位置只关注前后w个位置。

另一种情况是使用固定的模式，比如Stride模式，每隔几个位置关注一次。或者使用预定义的稀疏模式，比如只关注前面几个和后面几个位置。

在代码上，可能需要在计算attn矩阵之前生成一个mask矩阵，形状也是(B, num_heads, N, N)，其中需要被忽略的位置设为0或者一个很小的负数，然后在softmax之前应用这个mask。例如，对于每个query位置i，只允许j在i-w到i+w的位置，这样mask[i,j] = 0当|i-j|>w，否则为1。然后在计算attn时，attn = attn.masked_fill(mask == 0, -1e9)，这样softmax之后这些位置的权重就趋近于0。

或者，在计算的时候，直接限制每个query只处理对应的key。例如，对于每个头，可能将输入的k和v进行切片，只取相关的位置，从而减少计算量。但这种方法可能实现起来比较复杂，因为需要动态处理每个query对应的key范围。

另外，像BigBird中的稀疏注意力，可能结合了全局注意力、局部窗口和随机注意力，这时候的mask可能更复杂，需要组合不同的模式。

所以，代码上的主要差异可能集中在生成注意力权重的步骤，即如何计算QK^T，以及是否应用了某种mask或者限制key的范围。此外，还可能涉及到如何高效地实现这些稀疏计算，比如利用块稀疏矩阵运算，或者利用特定的库来加速。

比如，用户给出的普通Attention代码中，attn是完整的NxN矩阵，而sparse版本可能不会计算所有元素，或者在计算之后用mask过滤掉不需要的部分。但mask的方法在序列较长时仍然需要O(n²)的内存，虽然计算量可能减少，但内存占用可能还是问题。因此，更高效的实现可能需要改变计算方式，只计算必要的部分，例如使用滑动窗口或者分块处理。

举个例子，假设使用滑动窗口的sparse attention，每个query只关注前后w个位置。那么在代码中，可能需要为每个query生成对应的key和value的索引，然后进行局部计算。或者使用卷积的方式来处理，比如Depth-wise卷积来模拟局部注意力。

或者，像Sparse Transformer中使用固定的稀疏模式，使用二进制mask来指定哪些位置需要计算。这种情况下，mask会在初始化时确定，并在forward时应用到attn矩阵上。

总结一下，代码上的差异可能包括：

1. 引入mask矩阵，在计算注意力权重时应用，使得某些位置的权重被忽略。
2. 改变Q和K的计算方式，例如只取局部区域，减少计算量。
3. 可能使用不同的数据结构或运算来高效处理稀疏计算，如利用稀疏矩阵乘法。

现在结合用户提供的代码，普通Attention中的attn是完整的QK^T乘积。如果要改成sparse，比如局部窗口，可能需要生成一个mask，并在softmax前应用。例如：

在forward函数中，生成mask矩阵：

window_size = 5 # 假设窗口大小为5 mask = torch.ones((N, N), device=x.device) for i in range(N): start = max(0, i - window_size // 2) end = min(N, i + window_size // 2 + 1) mask[i, start:end] = 0 mask = mask.bool() attn = attn.masked_fill(mask, float('-inf'))

这样，只有窗口内的位置不被mask，其他位置被设为负无穷，softmax后权重接近0。

或者在计算QK^T时，只取对应的窗口内的K向量，这样无需计算全部的点积。但这样可能需要更复杂的索引操作。

另外，不同的sparse attention实现方式可能有不同的代码结构，比如使用不同的attention模式，或者利用现有的库如PyTorch的稀疏张量操作。

例如，使用torch.sparse模块来创建稀疏的注意力矩阵，但这种方法可能较为复杂，需要处理稀疏格式的转换。

另一个例子是Axial Attention，沿着不同的轴处理注意力，这时候可能需要重组张量的维度，并在特定维度上进行注意力计算。

另外，像Reformer的LSH attention，通过局部敏感哈希将相似的向量分到同一个桶中，只计算桶内的注意力，这需要完全不同的代码结构，包括哈希和排序步骤。

所以，具体到代码差异，可能要看sparse attention的具体类型。比如对于固定窗口的情况，主要差异在于mask的应用；对于基于哈希的，可能涉及分桶和重排序。

因此，在用户给出的普通Attention代码中，如果要改为sparse，比如局部窗口，则修改后的代码可能在计算attn之后应用一个mask，或者在计算时仅处理窗口内的元素。而如果是其他类型的sparse attention，则可能有不同的实现方式。

在代码实现上，稀疏注意力（Sparse Attention）与普通注意力（Full Attention）的主要差异集中在**注意力权重的计算方式**和**交互范围的限制**上。以下是具体差异点及代码示例：

------

### 1. **核心差异：注意力矩阵的计算**

#### 普通注意力 (Full Attention)

- **全连接计算**：每个查询（Query）与所有键（Key）计算点积，生成完整的注意力权重矩阵（`NxN`）。

- 代码片段

  ：

  ```python
  python
  
  复制代码attn = (q @ k.transpose(-2, -1)) * self.scale  # [B, H, N, N]
  ```

#### 稀疏注意力 (Sparse Attention)

- **局部计算或掩码限制**：每个查询仅与局部窗口/特定位置的键交互，或通过掩码屏蔽无关位置。

- 代码片段

  （以滑动窗口为例）：

  ```python
  python复制代码# 生成滑动窗口掩码（例如窗口大小=3）
  mask = torch.ones(N, N, dtype=torch.bool, device=x.device)
  for i in range(N):
      start = max(0, i-1)
      end = min(N, i+2)
      mask[i, start:end] = False
  # 应用掩码屏蔽非窗口位置
  attn = (q @ k.transpose(-2, -1)) * self.scale
  attn = attn.masked_fill(mask, float('-inf'))  # 被屏蔽位置权重趋近于0
  ```

------

### 2. **内存与计算效率优化**

#### 普通注意力

- **复杂度**：空间和时间复杂度均为 `O(N²)`，处理长序列时资源消耗大。
- **无优化**：直接存储完整的 `NxN` 矩阵。

#### 稀疏注意力

- **复杂度降低**：通过限制交互范围（如窗口大小 `w`），复杂度降至 `O(N*w)`。

- 高效实现

  ：可能使用块稀疏计算或索引操作，避免生成完整矩阵：

  ```python
  python复制代码# 示例：仅计算窗口内的键（非真实代码，示意逻辑）
  window_size = 3
  k_windows = sliding_window(k, window_size)  # [B, H, N, W, C/H]
  q_windows = sliding_window(q, window_size)  # [B, H, N, W, C/H]
  attn = (q_windows @ k_windows.transpose(-2, -1)) * self.scale  # [B, H, N, W]
  ```

------

### 3. **实现模式对比**

以下是两种注意力机制的代码结构对比：

| **步骤**              | **普通注意力**                           | **稀疏注意力**               |
| --------------------- | ---------------------------------------- | ---------------------------- |
| **QKV 投影**          | 相同：`self.qkv = nn.Linear(dim, dim*3)` | 相同                         |
| **注意力权重计算**    | 全连接：`q @ k.T`                        | 局部计算或掩码屏蔽           |
| **Softmax & Dropout** | 相同                                     | 相同（但部分位置权重被抑制） |
| **Value 加权求和**    | 全交互：`attn @ v`                       | 仅与有效位置的 `v` 交互      |

------

### 4. **典型稀疏注意力实现**

#### 示例1：滑动窗口注意力（如Swin Transformer）

```python
python复制代码def forward(self, x):
    B, N, C = x.shape
    qkv = self.qkv(x).reshape(...)  # 同普通注意力
    q, k, v = qkv[0], qkv[1], qkv[2]

    # 生成滑动窗口掩码（窗口大小=3）
    mask = torch.ones(N, N, dtype=torch.bool, device=x.device)
    for i in range(N):
        mask[i, max(0,i-1):min(N,i+2)] = False
    mask = mask.unsqueeze(0).unsqueeze(0)  # [1, 1, N, N]

    attn = (q @ k.transpose(-2, -1)) * self.scale
    attn = attn.masked_fill(mask, float('-inf'))  # 屏蔽非窗口位置
    attn = attn.softmax(dim=-1)
    # 后续步骤同普通注意力...
```

#### 示例2：块稀疏注意力（如BigBird）

```python
python复制代码# 使用块掩码（Block Sparse Mask）
block_size = 32  # 块大小
num_blocks = N // block_size
mask = torch.zeros(num_blocks, num_blocks, dtype=torch.bool)
# 定义块之间的连接规则（如随机、局部、全局）
mask = create_block_sparse_mask(num_blocks, mode="local+random")
mask = mask.repeat_interleave(block_size, dim=0).repeat_interleave(block_size, dim=1)
attn = attn.masked_fill(mask, float('-inf'))
```

------

### 总结

- **普通注意力**：完整的 `NxN` 交互，计算和内存成本高。
- **稀疏注意力**：通过掩码或局部计算限制交互范围，显著降低复杂度，但需根据任务设计稀疏模式（如滑动窗口、哈希分桶、块稀疏等）。代码差异主要体现在注意力矩阵生成时的掩码或索引操作。

## 金字塔模型结构分析





![image-20250224120040462](C:\Users\Ste'fan\AppData\Roaming\Typora\typora-user-images\image-20250224120040462.png)

![image-20250226103842007](C:\Users\Ste'fan\AppData\Roaming\Typora\typora-user-images\image-20250226103842007.png)

具体来说，首先执行自上而下的特征融合操作如下：
其中，C（）表示特征过滤的卷积操作，Up（）表示上采样。接下来，
底向上的特征融合操作如下：

![image-20250226103948217](C:\Users\Ste'fan\AppData\Roaming\Typora\typora-user-images\image-20250226103948217.png)

其中，ME（·）表示加权的ME [16] ，它将自动学习每个嵌入对应的权重，并将其加权以获得最终的嵌入。Down（·）表示下采样。MaxPooling（·）表示最大池化。

代码参考：

https://blog.csdn.net/thehappysheep/article/details/121095909?spm=1001.2101.3001.6650.14&utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromBaidu%7ERate-14-121095909-blog-128914225.235%5Ev43%5Econtrol&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromBaidu%7ERate-14-121095909-blog-128914225.235%5Ev43%5Econtrol&utm_relevant_index=15

![image-20250226111724119](C:\Users\Ste'fan\AppData\Roaming\Typora\typora-user-images\image-20250226111724119.png)

1. **ResNet-34 Backbone**:

   - 图像的左侧部分显示了一个ResNet-34网络的主干部分（backbone）。ResNet-34是一个深度残差网络，用于提取图像的特征表示。
   - ResNet-34的输出是不同尺度的特征图（feature maps），这些特征图通常具有不同的分辨率和语义信息。

2. **特征金字塔（Feature Pyramid）**:

   - 图像的右侧部分显示了一个特征金字塔结构。特征金字塔通过结合不同尺度的特征图来生成一个多尺度的特征表示。
   - 特征金字塔的构建过程包括以下步骤：
     - **自底向上路径（Bottom-Up Pathway）**: ResNet-34的主干网络生成不同尺度的特征图（如`F_out1`, `F_out2`, `F_out3`, `F_out4`）。
     - **自顶向下路径（Top-Down Pathway）**: 从最高层的特征图开始，通过上采样（如`P4`到`P3`）逐步生成低层的特征图。
     - **横向连接（Lateral Connections）**: 将自顶向下路径的特征图与自底向上路径的特征图进行融合（如`P4`与`F_out3`融合生成`P3`）。
     - **特征融合（Feature Fusion）**: 通过卷积操作（如`B1`, `B2`, `B3`, `B4`）对融合后的特征图进行进一步处理，生成最终的多尺度特征图。

3. **输出特征图**:

   - 特征金字塔的输出是多个尺度的特征图（如`P1`, `P2`, `P3`, `P4`, `P5`），这些特征图可以用于多尺度的目标检测或其他计算机视觉任务。

   

   ### 特征金字塔是什么

   特征金字塔（Feature Pyramid）是深度学习中用于构建多尺度特征表示的一种结构，尤其在目标检测、图像分割等计算机视觉任务中广泛应用。其核心思想是通过结合不同层次的特征图，生成一个多尺度的特征金字塔，每个尺度的特征图都适合检测特定尺寸的目标。特征金字塔的主要组成部分包括：

   1. **自底向上路径（Bottom-Up Pathway）**：这是标准的卷积神经网络路径，通过连续的卷积和池化操作，逐渐降低特征图的空间分辨率，同时增加特征图的深度，从而提取图像的高层语义信息。
   2. **自顶向下路径（Top-Down Pathway）**：通过将深层的高语义特征图进行上采样（如使用反卷积或双线性插值），增加其空间分辨率，使其更适合检测小尺寸目标。
   3. **横向连接（Lateral Connections）**：在自顶向下路径中，将上采样的深层特征图与相应尺度的浅层特征图通过1x1卷积融合，以增强特征图的语义信息。
   4. **特征融合**：通过上述结构，特征金字塔网络能够在不同尺度的特征图上同时保留丰富的语义信息和细节信息，从而有效地检测不同尺寸的目标。

   ### Python代码构建特征金字塔

   以下是使用PyTorch构建特征金字塔网络（FPN）的示例代码：

   ```python
   import torch
   import torch.nn as nn
   import torch.nn.functional as F
   
   class FPNNeck(nn.Module):
       def __init__(self, in_channels, out_channels):
           super(FPNNeck, self).__init__()
           # Lateral convolutions to reduce channel dimensions
           self.lateral_convs = nn.ModuleList([
               nn.Conv2d(in_channels[i], out_channels, 1) for i in range(len(in_channels))
           ])
           # Smooth convolutions to smooth the combined features
           self.fpn_convs = nn.ModuleList([
               nn.Conv2d(out_channels, out_channels, 3, padding=1) for _ in range(len(in_channels))
           ])
   
       def forward(self, inputs):
           # Build laterals
           laterals = [lateral_conv(inputs[i]) for i, lateral_conv in enumerate(self.lateral_convs)]
           
           # Build top-down path
           for i in range(len(laterals) - 1, 0, -1):
               laterals[i - 1] += F.interpolate(laterals[i], scale_factor=2, mode='bilinear', align_corners=False)
           
           # Build outputs
           outs = [fpn_conv(lateral) for fpn_conv, lateral in zip(self.fpn_convs, laterals)]
           return outs
   
   class ResNetFPN(nn.Module):
       def __init__(self, backbone, in_channels, out_channels, num_classes):
           super(ResNetFPN, self).__init__()
           self.backbone = backbone
           self.fpn_neck = FPNNeck(in_channels, out_channels)
           self.head = nn.Linear(out_channels, num_classes)
   
       def forward(self, x):
           # Extract features from backbone
           features = self.backbone(x)
           # Process features through FPN neck
           fpn_features = self.fpn_neck(features)
           # Example: Use the first FPN level for classification
           pooled = F.adaptive_avg_pool2d(fpn_features[0], (1, 1)).view(fpn_features[0].size(0), -1)
           return self.head(pooled)
   ```

   ### 代码说明

   1. **`FPNNeck`类**：实现了特征金字塔的核心架构，包括横向卷积（`lateral_convs`）和特征平滑卷积（`fpn_convs`）。
   2. **`ResNetFPN`类**：将ResNet作为骨干网络（`backbone`），结合FPN颈部网络（`fpn_neck`）和分类头（`head`），完成从输入图像到分类输出的完整流程。
   3. **前向传播**：输入图像通过骨干网络提取特征，然后通过FPN颈部网络生成多尺度特征金字塔，最后通过分类头进行预测。

   通过上述代码，你可以构建一个简单的特征金字塔网络，用于多尺度特征提取和目标检测等任务。

   

   ### ME:**Multi-Value Embedding

   图中的ME表示**Multi-Value Embedding**，即多值嵌入。以下是对它的详细解释：

   - **Multi-Value Embedding** 是一种嵌入技术，用于处理具有多个值的特征（多值特征）。它将多个离散值映射到一个低维稠密向量空间中，使得这些向量能够表达相应对象的某些特征，同时向量之间的距离能反映对象之间的相似性。
   - 在推荐系统中，多值特征常见于用户的行为序列、商品的属性描述等。例如，一个用户的浏览历史可能包含多个商品ID，这些ID可以视为多值特征。

   ### 应用
   - **推荐系统**：用于处理用户的行为序列、商品的多属性等多值特征，以捕捉用户的兴趣偏好和商品的特性。
   - **自然语言处理**：用于处理文本中的多义词、多关键词等多值特征，以更好地理解文本的语义。
   - **计算机视觉**：用于处理图像中的多标签、多区域特征等多值特征，以提高模型的表达能力和泛化能力。

   ### 特征金字塔中的作用
   - 在特征金字塔模型中，Multi-Value Embedding 可以用于处理不同尺度的多值特征，将其映射到统一的向量空间中，从而更好地融合多尺度特征，提高模型的性能。
   - 例如，在目标检测任务中，特征金字塔的不同层级可能包含不同尺度的目标特征，通过 Multi-Value Embedding，可以将这些特征有效地融合，生成更丰富的特征表示。

   ### 实现方法
   - **基础 Multi-Value Embedding**：对多值特征中的每个值进行单独的嵌入，然后通过取均值、最大值、最小值等统计方法，得到最终的多值特征表示。
   - **加权 Multi-Value Embedding**：在基础方法的基础上，自动学习每个嵌入向量的权重，然后进行加权求和，得到最终的多值特征表示。
   - **Transformer-based Embedding**：利用 Transformer 的自注意力机制，对多值特征进行嵌入，从而捕捉多值特征之间的复杂关系。

   ### 优势
   - **捕捉复杂关系**：能够有效地捕捉多值特征之间的复杂关系，提高模型的表达能力。
   - **提高泛化能力**：通过将多值特征映射到低维稠密向量空间中，减少特征的稀疏性，提高模型的泛化能力。
   - **适用于多种任务**：在推荐系统、自然语言处理、计算机视觉等多种任务中都有广泛的应用。

   ### 示例代码
   以下是一个简单的 Multi-Value Embedding 的实现示例（基于 PyTorch）：

   ```python
   import torch
   import torch.nn as nn
   
   class MultiValueEmbedding(nn.Module):
       def __init__(self, vocab_size, embedding_dim):
           super(MultiValueEmbedding, self).__init__()
           self.embedding = nn.Embedding(vocab_size, embedding_dim)
           self.weight = nn.Parameter(torch.randn(embedding_dim))
   
       def forward(self, input_values):
           # input_values: [batch_size, num_values]
           embedded_values = self.embedding(input_values)  # [batch_size, num_values, embedding_dim]
           weighted_values = embedded_values * self.weight  # [batch_size, num_values, embedding_dim]
           final_embedding = torch.sum(weighted_values, dim=1)  # [batch_size, embedding_dim]
           return final_embedding
   
   # 示例
   vocab_size = 10000
   embedding_dim = 50
   batch_size = 32
   num_values = 10
   
   model = MultiValueEmbedding(vocab_size, embedding_dim)
   input_values = torch.randint(0, vocab_size, (batch_size, num_values))
   output = model(input_values)
   print(output.shape)  # 输出: torch.Size([32, 50])
   ```

   ### 总结
   Multi-Value Embedding 是一种强大的嵌入技术，能够有效地处理多值特征，广泛应用于推荐系统、自然语言处理和计算机视觉等领域。在特征金字塔模型中，它可以帮助融合多尺度的多值特征，提高模型的性能和泛化能力。

## ADD模块

### 结构说明

根据描述和图示，该网络的结构可以分为以下几个部分：

1. **输入特征**:
   - 输入特征（B1–B6）是特征金字塔的输出特征图，通常表示为多尺度的特征向量。

2. **全连接层（Fully Connected Layer）**:
   - 输入特征被插入到一个全连接层（FC层），用于特征的进一步提取和变换。
   - 全连接层的输出是一个更高维度的特征向量，用于指导后续的精确分类和回归任务。

3. **Selective Kernel Convolution (SKC)**:
   - 全连接层的输出特征图被输入到SKC模块。
   - SKC模块通过动态选择卷积核大小，自适应地调整感受野，从而更好地捕捉多尺度特征。
   - SKC模块的输出是经过增强的特征图，用于分类和回归任务。

4. **分类和回归分支**:
   - **分类分支 (Cls-score)**:
     - 通过全连接层或其他分类头，对SKC模块的输出特征图进行分类，输出每个目标的类别分数。
   - **回归分支 (Bbox-pred)**:
     - 通过回归头，预测目标的边界框位置和大小。

5. **损失函数**:
   - 最终的回归损失可以通过以下损失函数计算：
     \[
     \mathcal{L} = \mathcal{L}_{\text{cls}} + \lambda \mathcal{L}_{\text{reg}}
     \]
     其中，\(\mathcal{L}_{\text{cls}}\) 是分类损失，\(\mathcal{L}_{\text{reg}}\) 是回归损失，\(\lambda\) 是权重参数。

### SKC相关代码

以下是基于PyTorch的代码实现：

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class SKC(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_sizes=[3, 5, 7]):
        super(SKC, self).__init__()
        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_sizes[0], padding=kernel_sizes[0]//2)
        self.conv2 = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_sizes[1], padding=kernel_sizes[1]//2)
        self.conv3 = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_sizes[2], padding=kernel_sizes[2]//2)
        self.conv_selection = nn.Conv2d(out_channels, out_channels, kernel_size=1)
        self.softmax = nn.Softmax(dim=1)

    def forward(self, x):
        # Multi-scale convolutions
        feat1 = self.conv1(x)
        feat2 = self.conv2(x)
        feat3 = self.conv3(x)
        
        # Concatenate features
        feats = torch.cat([feat1, feat2, feat3], dim=1)
        
        # Selective kernel selection
        selection = self.conv_selection(feats)
        selection = self.softmax(selection)
        
        # Weighted sum
        output = feats * selection
        return output

class ADDModule(nn.Module):
    def __init__(self, input_features, hidden_units, output_classes):
        super(ADDModule, self).__init__()
        # Fully connected layer
        self.fc = nn.Linear(input_features, hidden_units)
        
        # SKC module
        self.skc = SKC(in_channels=hidden_units, out_channels=hidden_units)
        
        # Classification head
        self.cls_head = nn.Linear(hidden_units, output_classes)
        
        # Regression head
        self.reg_head = nn.Linear(hidden_units, 4)  # Assuming bounding box regression

    def forward(self, x):
        # Fully connected layer
        x = F.relu(self.fc(x))
        
        # SKC module
        x = self.skc(x)
        
        # Classification and regression heads
        cls_score = self.cls_head(x)
        bbox_pred = self.reg_head(x)
        
        return cls_score, bbox_pred

# Example usage
if __name__ == "__main__":
    # Input features (B1-B6)
    input_features = 6  # Example: 6 input features
    hidden_units = 128  # Hidden units in the fully connected layer
    output_classes = 10  # Example: 10 object classes

    model = ADDModule(input_features, hidden_units, output_classes)
    x = torch.randn(1, input_features)  # Example input
    cls_score, bbox_pred = model(x)
    print("Classification scores:", cls_score.shape)
    print("Bounding box predictions:", bbox_pred.shape)
```

### 代码说明

1. **SKC模块**:
   - `SKC` 类实现了Selective Kernel Convolution模块，包括多尺度卷积、动态权重计算和加权求和。

2. **ADD模块**:
   - `ADDModule` 类实现了整个网络结构，包括全连接层、SKC模块、分类头和回归头。

3. **前向传播**:
   - 输入特征通过全连接层进行特征提取，然后通过SKC模块进行多尺度特征增强，最后通过分类头和回归头分别输出分类分数和边界框预测。

4. **损失函数**:
   - 可以使用交叉熵损失（分类）和均方误差损失（回归）的组合来训练模型。

### 总结

该网络结构通过全连接层和SKC模块，实现了特征的逐步提取和增强，最终通过分类和回归分支输出目标的类别和位置信息。代码实现了上述结构，并提供了示例用法。

### 更完备的SKC代码

```python
import torch
from torch import nn
 
# conv = SKConv(64, 32, 3, 8, 2)
class SKConv(nn.Module):
    def __init__(self, features, WH, M, G, r, stride=1, L=32):
        """ Constructor
        Args:
            features: 输入通道维度
            WH: 输入特征图的空间维度
            M: 分支的数量
            G: 卷积组的数量
            r: 计算d，向量s的压缩倍数，C/r
            stride: 步长，默认为1
            L: 矢量z的最小维度，默认为32
        """
        super(SKConv, self).__init__()
        d = max(int(features / r), L)
        self.M = M
        self.features = features
        self.convs = nn.ModuleList([])
        # 使用不同kernel size的卷积，增加不同的感受野
        for i in range(M):
            self.convs.append(nn.Sequential(
                nn.Conv2d(features, features, kernel_size=3 + i * 2, stride=stride, padding=1 + i, groups=G),
                nn.BatchNorm2d(features),
                nn.ReLU(inplace=False)
            ))
        # 全局平均池化
        self.gap = nn.AvgPool2d(int(WH / stride))
        self.fc = nn.Linear(features, d)
        self.fcs = nn.ModuleList([])
        # 全连接层
        for i in range(M):
            self.fcs.append(
                nn.Linear(d, features)
            )
        self.softmax = nn.Softmax(dim=1)
 
    def forward(self, x):
        ''' Split操作'''
        for i, conv in enumerate(self.convs):
            fea = conv(x).unsqueeze_(dim=1)
            if i == 0:
                feas = fea
            else:
                feas = torch.cat([feas, fea], dim=1)
 
        ''' Fuse操作'''
        fea_U = torch.sum(feas, dim=1)
        fea_s = self.gap(fea_U).squeeze_()
        fea_z = self.fc(fea_s)
 
        ''' Select操作'''
        for i, fc in enumerate(self.fcs):
            # fc-->d*c维
            vector = fc(fea_z).unsqueeze_(dim=1)
            if i == 0:
                attention_vectors = vector
            else:
                attention_vectors = torch.cat([attention_vectors, vector], dim=1)
        # 计算attention权重
        attention_vectors = self.softmax(attention_vectors)
        attention_vectors = attention_vectors.unsqueeze(-1).unsqueeze(-1)
        # 最后一步，各特征图与对应的注意力权重相乘，得到输出特征图V
        fea_v = (feas * attention_vectors).sum(dim=1)
        return fea_v
 
 
class SKUnit(nn.Module):
    def __init__(self, in_features, out_features, WH, M, G, r, mid_features=None, stride=1, L=32):
        """ Constructor
        Args:
            in_features: 输入通道维度
            out_features: 输出通道维度
            WH: 输入特征图的空间维度
            M: 分支的数量
            G: 卷积组的数量
            r: 计算d，论文中向量s的压缩倍数，C/r
            mid_features: 步长不为1的中间卷积的通道维度，默认为out_features/2
            stride: 步长，默认为1
            L: 论文中矢量z的最小维度，默认为32
        """
        super(SKUnit, self).__init__()
        if mid_features is None:
            mid_features = int(out_features / 2)
        self.feas = nn.Sequential(
            nn.Conv2d(in_features, mid_features, 1, stride=1),
            nn.BatchNorm2d(mid_features),
            # SKConv替代了ResNext中3*3卷积部分
            SKConv(mid_features, WH, M, G, r, stride=stride, L=L),
            nn.BatchNorm2d(mid_features),
            nn.Conv2d(mid_features, out_features, 1, stride=1),
            nn.BatchNorm2d(out_features)
        )
        if in_features == out_features:  # when dim not change, in could be added diectly to out
            self.shortcut = nn.Sequential()
        else:  # when dim not change, in should also change dim to be added to out
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_features, out_features, 1, stride=stride),
                nn.BatchNorm2d(out_features)
            )
 
    def forward(self, x):
        fea = self.feas(x)
        return fea + self.shortcut(x)
 
 
class SKNet(nn.Module):
    def __init__(self, class_num):
        super(SKNet, self).__init__()
        self.basic_conv = nn.Sequential(
            nn.Conv2d(3, 64, 3, padding=1),
            nn.BatchNorm2d(64)
        )  # 32x32
        self.stage_1 = nn.Sequential(
            SKUnit(64, 256, 32, 2, 8, 2, stride=2),
            nn.ReLU(),
            SKUnit(256, 256, 32, 2, 8, 2),
            nn.ReLU(),
            SKUnit(256, 256, 32, 2, 8, 2),
            nn.ReLU()
        )  # 32x32
        self.stage_2 = nn.Sequential(
            SKUnit(256, 512, 32, 2, 8, 2, stride=2),
            nn.ReLU(),
            SKUnit(512, 512, 32, 2, 8, 2),
            nn.ReLU(),
            SKUnit(512, 512, 32, 2, 8, 2),
            nn.ReLU()
        )  # 16x16
        self.stage_3 = nn.Sequential(
            SKUnit(512, 1024, 32, 2, 8, 2, stride=2),
            nn.ReLU(),
            SKUnit(1024, 1024, 32, 2, 8, 2),
            nn.ReLU(),
            SKUnit(1024, 1024, 32, 2, 8, 2),
            nn.ReLU()
        )  # 8x8
        self.pool = nn.AvgPool2d(8)
        self.classifier = nn.Sequential(
            nn.Linear(1024, class_num),
            # nn.Softmax(dim=1)
        )
 
    def forward(self, x):
        fea = self.basic_conv(x)
        fea = self.stage_1(fea)
        fea = self.stage_2(fea)
        fea = self.stage_3(fea)
        fea = self.pool(fea)
        fea = torch.squeeze(fea)
        fea = self.classifier(fea)
        return fea
 
 
if __name__ == '__main__':
    # 随机生成8个（64，32，32）的特征图
    x = torch.rand(8, 64, 32, 32)
    conv = SKConv(64, 32, 3, 8, 2)
    out = conv(x)
    criterion = nn.L1Loss()
    loss = criterion(out, x)
    loss.backward()
    # 最终输出特征图V的size和损失值
    print('out shape : {}'.format(out.shape))
    print('loss value : {}'.format(loss))
```

### Unit->Net的必要性

#### 1. **模块化设计**
- **`SKConv`** 是一个基础模块，用于实现选择性核卷积（Selective Kernel Convolution）的功能。
- **`SKUnit`** 是一个更高级的模块，通常包含一个或多个 `SKConv` 模块，以及其他必要的层（如 1x1 卷积、BatchNorm、ReLU 等），用于构建残差单元（Residual Unit）。
- **`SKNet`** 是一个完整的网络，由多个 `SKUnit` 模块组成，用于实现特定的任务（如图像分类、目标检测等）。

通过模块化设计，可以更好地组织代码，提高代码的可读性和可维护性。

#### 2. **残差连接**
- **`SKUnit`** 通常包含残差连接（Residual Connection），这是 ResNet 和其他深度网络中的一个重要设计。残差连接可以缓解梯度消失问题，使网络更容易训练。
- 直接调用 `SKConv` 时，通常需要手动添加残差连接，而 `SKUnit` 已经封装了这些细节。

#### 3. **网络深度和复杂性**
- **`SKNet`** 通过堆叠多个 `SKUnit` 模块，可以构建一个更深的网络，从而提高模型的表达能力和性能。
- 直接调用 `SKConv` 时，通常只能实现一个较浅的网络，可能无法满足复杂任务的需求。

#### 4. **灵活性和扩展性**
- **`SKUnit`** 和 **`SKNet`** 提供了更高的灵活性和扩展性，可以根据任务需求调整网络的深度、宽度和结构。
- 例如，可以通过调整 `SKUnit` 中的参数（如 `M`、`G`、`r` 等）来控制网络的复杂度和性能。

#### 5. **代码复用**
- **`SKUnit`** 和 **`SKNet`** 可以在不同的任务和数据集上复用，而不需要每次都重新编写代码。
- 例如，可以在图像分类、目标检测、语义分割等任务中使用相同的 `SKUnit` 和 `SKNet` 模块。

#### 6. **实际应用**
- 在实际应用中，通常需要一个完整的网络结构（如 `SKNet`）来完成特定的任务。直接调用 `SKConv` 只能实现一个基础的功能，无法满足复杂任务的需求。

### 示例

假设你有一个图像分类任务，需要构建一个完整的网络。直接调用 `SKConv` 可能无法满足需求，而使用 `SKUnit` 和 `SKNet` 可以更方便地构建一个完整的网络。

```python
# 直接调用 SKConv
conv = SKConv(64, 32, 3, 8, 2)
x = torch.rand(8, 64, 32, 32)
out = conv(x)

# 使用 SKUnit 和 SKNet
net = SKNet(class_num=1000)
x = torch.rand(8, 3, 224, 224)
out = net(x)
```

### 总结

- **直接调用 `SKConv`**：适用于简单的任务或作为基础模块使用。
- **使用 `SKUnit` 和 `SKNet`**：适用于更复杂的任务，提供更高的灵活性、扩展性和代码复用性。

通过模块化设计，可以更好地组织代码，提高代码的可读性和可维护性，同时满足复杂任务的需求。

全连接层

全连接层（Fully Connected Layer，也称为 Dense Layer）是神经网络中的一种常见层结构，通常用于将特征从一个空间映射到另一个空间，以实现分类或回归任务。以下是全连接层的具体工作原理：

##  **全连接层的定义**
- **全连接层**是神经网络中的一种层结构，其中每个神经元都与前一层的所有神经元相连。
- 全连接层通常用于神经网络的最后几层，用于将特征从高维空间映射到低维空间（如分类任务中的类别数）。

### 2. **全连接层的工作原理**
全连接层的主要工作原理是通过矩阵乘法和加法，将输入特征映射到输出特征。具体步骤如下：

#### 输入和输出
- **输入**: 全连接层的输入是一个一维向量（通常通过展平操作将多维特征图转换为一维向量）。
- **输出**: 全连接层的输出也是一个一维向量，其维度由设计决定（如分类任务中的类别数）。

#### 矩阵乘法
- 假设输入向量为 \(\mathbf{x} \in \mathbb{R}^{n}\)，权重矩阵为 \(\mathbf{W} \in \mathbb{R}^{m \times n}\)，偏置向量为 \(\mathbf{b} \in \mathbb{R}^{m}\)。
- 全连接层的输出 \(\mathbf{y} \in \mathbb{R}^{m}\) 通过以下公式计算：
  \[
  \mathbf{y} = \mathbf{W} \mathbf{x} + \mathbf{b}
  \]
  其中：
  - \(\mathbf{W}\) 是权重矩阵，用于将输入特征映射到输出特征。
  - \(\mathbf{b}\) 是偏置向量，用于调整输出的偏移量。

#### 激活函数
- 在计算输出后，通常会通过一个激活函数（如 ReLU、Sigmoid 或 Tanh）来引入非线性，从而增强模型的表达能力。
  \[
  \mathbf{y} = f(\mathbf{W} \mathbf{x} + \mathbf{b})
  \]
  其中 \(f\) 是激活函数。

### 3. **全连接层的参数**
- **权重矩阵 \(\mathbf{W}\)**: 用于学习输入特征和输出特征之间的映射关系。
- **偏置向量 \(\mathbf{b}\)**: 用于调整输出的偏移量。
- **参数数量**: 全连接层的参数数量为 \(n \times m + m\)，其中 \(n\) 是输入特征的维度，\(m\) 是输出特征的维度。

### 4. **全连接层的作用**
- **特征映射**: 将输入特征从一个空间映射到另一个空间，通常用于分类或回归任务。
- **信息整合**: 通过权重矩阵和偏置向量，整合输入特征的信息，提取更高层次的特征表示。
- **非线性变换**: 通过激活函数引入非线性，使模型能够学习复杂的模式。

### 5. **代码实现**
以下是使用 PyTorch 实现全连接层的示例代码：

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

# 定义全连接层
class FullyConnectedLayer(nn.Module):
    def __init__(self, input_features, output_features):
        super(FullyConnectedLayer, self).__init__()
        self.fc = nn.Linear(input_features, output_features)

    def forward(self, x):
        # 展平输入
        x = x.view(x.size(0), -1)
        # 全连接层计算
        x = self.fc(x)
        # 激活函数
        x = F.relu(x)
        return x

# 示例
if __name__ == "__main__":
    # 输入特征维度和输出特征维度
    input_features = 1024
    output_features = 512

    # 创建全连接层
    fc_layer = FullyConnectedLayer(input_features, output_features)

    # 输入数据
    x = torch.randn(32, 1024)  # 32个样本，每个样本1024个特征

    # 前向传播
    output = fc_layer(x)
    print(output.shape)  # 输出: torch.Size([32, 512])
```

### 6. **代码说明**
1. **`nn.Linear`**:
   - `nn.Linear(input_features, output_features)` 定义了一个全连接层，其中 `input_features` 是输入特征的维度，`output_features` 是输出特征的维度。
2. **展平操作**:
   - `x.view(x.size(0), -1)` 将输入数据展平为一维向量，以便输入到全连接层。
3. **激活函数**:
   - 使用 `F.relu(x)` 对全连接层的输出应用 ReLU 激活函数，引入非线性。

### 7. **总结**
全连接层通过矩阵乘法和加法，将输入特征映射到输出特征，并通过激活函数引入非线性。它是神经网络中用于特征映射和信息整合的重要层结构，广泛应用于分类和回归任务中。



## **Feedforward**

**Feedforward**（前馈）是神经网络中的一种基本结构，其中数据从输入层经过一个或多个隐藏层，最终到达输出层，整个过程中数据只在一个方向上流动（从输入到输出）。前馈网络通常用于构建各种类型的神经网络，如多层感知机（MLP）、卷积神经网络（CNN）和Transformer中的前馈网络（Feed-Forward Network, FFN）。

在Transformer架构中，**Feed-Forward Network (FFN)** 是每个Transformer块中的一个重要组件，用于对注意力机制的输出进行进一步的特征提取和变换。***<u>FFN通常由两个全连接层和一个激活函数组成。</u>***

### 2. **Feedforward 的结构**

一个典型的FFN包含以下部分：
1. **输入层**：接收来自上一层（如注意力机制的输出）的特征向量。
2. **第一个全连接层**：将输入特征映射到一个隐藏空间，通常具有较大的隐藏维度。
3. **激活函数**：对隐藏层的输出应用非线性激活函数（如ReLU、GELU等）。
4. **第二个全连接层**：将激活后的特征映射回原始特征维度。
5. **输出层**：生成最终的输出特征向量。

### 3. **FFN 的作用**

1. **特征提取**：通过非线性变换，提取更高级的特征表示。
2. **增加模型容量**：通过增加网络的深度和宽度，提高模型的表达能力。
3. **缓解过拟合**：通过Dropout等正则化技术，防止模型过拟合。

### 4. **PyTorch 实现 FFN**

以下是一个简单的FFN实现，通常用于Transformer中的前馈网络：

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class FeedForwardNetwork(nn.Module):
    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):
        super().__init__()
        out_features = out_features or in_features
        hidden_features = hidden_features or in_features
        drop_probs = (drop, drop)

        self.fc1 = nn.Linear(in_features, hidden_features)
        self.act = act_layer()
        self.drop1 = nn.Dropout(drop_probs[0])
        self.fc2 = nn.Linear(hidden_features, out_features)
        self.drop2 = nn.Dropout(drop_probs[1])

    def forward(self, x):
        x = self.fc1(x)
        x = self.act(x)
        x = self.drop1(x)
        x = self.fc2(x)
        x = self.drop2(x)
        return x
```

### 5. **代码解析**

#### **初始化方法 (`__init__`)**
```python
def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):
    super().__init__()
    out_features = out_features or in_features
    hidden_features = hidden_features or in_features
    drop_probs = (drop, drop)

    self.fc1 = nn.Linear(in_features, hidden_features)
    self.act = act_layer()
    self.drop1 = nn.Dropout(drop_probs[0])
    self.fc2 = nn.Linear(hidden_features, out_features)
    self.drop2 = nn.Dropout(drop_probs[1])
```

##### 参数解释：
- **`in_features`**: 输入特征的维度。
- **`hidden_features`**: 隐藏层的维度。如果未指定，则默认为 `in_features`。
- **`out_features`**: 输出特征的维度。如果未指定，则默认为 `in_features`。
- **`act_layer`**: 激活函数，默认为 `nn.GELU`。
- **`drop`**: Dropout 比例。

##### 初始化过程：
1. **`out_features` 和 `hidden_features`**:
   - 如果未指定 `out_features` 或 `hidden_features`，则默认为 `in_features`。
2. **`drop_probs`**:
   - Dropout 比例，应用于两个 Dropout 层。
3. **`self.fc1`**:
   - 第一个全连接层，将输入特征从 `in_features` 映射到 `hidden_features`。
4. **`self.act`**:
   - 激活函数层。
5. **`self.drop1`**:
   - 第一个 Dropout 层，应用于激活函数的输出。
6. **`self.fc2`**:
   - 第二个全连接层，将隐藏层特征从 `hidden_features` 映射到 `out_features`。
7. **`self.drop2`**:
   - 第二个 Dropout 层，应用于第二个全连接层的输出。

#### **前向传播 (`forward`)**
```python
def forward(self, x):
    x = self.fc1(x)
    x = self.act(x)
    x = self.drop1(x)
    x = self.fc2(x)
    x = self.drop2(x)
    return x
```

##### 前向传播过程：
1. **`self.fc1(x)`**:
   - 输入特征 `x` 通过第一个全连接层，映射到隐藏层特征。
2. **`self.act(x)`**:
   - 对隐藏层特征应用激活函数。
3. **`self.drop1(x)`**:
   - 对激活后的特征应用第一个 Dropout 层。
4. **`self.fc2(x)`**:
   - 通过第二个全连接层，将隐藏层特征映射到输出特征。
5. **`self.drop2(x)`**:
   - 对输出特征应用第二个 Dropout 层。
6. **返回结果**:
   - 返回最终的输出特征 `x`。

### 6. **示例用法**

以下是一个完整的示例，展示如何使用这个FFN：

```python
import torch

# 输入特征 (Batch size=2, Sequence length=10, Feature dimension=64)
x = torch.randn(2, 10, 64)

# 创建FFN (in_features=64, hidden_features=256, out_features=64)
ffn = FeedForwardNetwork(in_features=64, hidden_features=256)

# 前向传播
output = ffn(x)
print(output.shape)  # 输出: torch.Size([2, 10, 64])
```

### 7. **总结**

- **Feedforward Network (FFN)** 是Transformer中的一个重要组件，用于对注意力机制的输出进行进一步的特征提取和变换。
- FFN 通常由两个全连接层和一个激活函数组成，能够增加模型的非线性表达能力。
- 通过Dropout等正则化技术，可以防止模型过拟合。
- 以上代码实现了一个通用的FFN，可以用于各种Transformer相关的任务。

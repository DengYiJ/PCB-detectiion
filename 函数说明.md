## Dataloader

在 PyTorch 中，`DataLoader` 是一个迭代器，它封装了你的数据集（`Dataset` 实例），并提供了一种便捷的方式来迭代数据集中的数据。当你使用 `DataLoader` 并结合 `for` 循环来迭代时，`DataLoader` 会自动从它所封装的数据集中按批次取出数据，并将其作为 `x_batch` 和 `y_batch` 返回。

这里的 `x_batch` 和 `y_batch` 不是新制造的变量，而是在每次迭代中由 `DataLoader` 返回的两个张量（Tensor），它们分别代表一批数据的特征和标签。

具体来说：

- `x_batch` 是一批数据的特征，它的形状取决于你的数据集如何定义 `__getitem__` 方法。通常，如果你的数据集是图像，`x_batch` 可能包含多个图像的张量；如果是文本数据，它可能包含经过处理的文本特征向量。

- `y_batch` 是对应的标签，它是一个一维张量，包含了 `x_batch` 中每个样本的标签。

这个过程是 `DataLoader` 自带的，不需要你手动实现。`DataLoader` 会在每次迭代中自动调用你的数据集的 `__getitem__` 方法，并将返回的结果作为 `x_batch` 和 `y_batch` 提供给你。

下面是一个简化的例子来说明这个过程：

```python
from torch.utils.data import DataLoader, Dataset

class MyDataset(Dataset):
    def __init__(self):
        # 假设我们有一些数据
        self.data = [1, 2, 3, 4, 5]
        self.targets = [10, 20, 30, 40, 50]

    def __len__(self):
        # 返回数据集中的样本数量
        return len(self.data)

    def __getitem__(self, idx):
        # 返回一个样本及其标签
        return self.data[idx], self.targets[idx]

# 创建数据集实例
dataset = MyDataset()

# 创建数据加载器
data_loader = DataLoader(dataset, batch_size=2, shuffle=True)

# 迭代数据加载器
for x_batch, y_batch in data_loader:
    print(f"Features: {x_batch}, Labels: {y_batch}")
```

在这个例子中，`DataLoader` 会从 `MyDataset` 数据集中取出数据，并在每次迭代中返回两个批次的特征和标签。这里的 `x_batch` 和 `y_batch` 就是从数据集中取出的样本数据和标签。
对于监督学习任务：

## 标签与监督学习

标签通常是人工标注的，例如在图像识别任务中，每张图片可能都有一个与之对应的类别标签。
在自然语言处理任务中，文本数据可能被标注为情感分类（正面、负面）或其他类别。
对于无监督学习任务：

可能没有明确的标签，模型需要自行发现数据中的模式。
对于半监督学习任务：

部分数据有标签，部分数据没有标签。
在你的例子中，self.targets 只是一个简单的数字列表，用于演示如何创建一个简单的数据集。在实际应用中，标签应该根据你的任务需求来确定。例如，如果你正在处理一个分类问题，标签可能是类别名称或类别编号；如果是回归问题，标签可能是连续的数值。

如果你需要生成随机标签，你可以使用 Python 的 random 模块来生成随机数或随机选择预定义标签集中的标签。但请注意，随机生成的标签可能不适用于实际的机器学习任务，因为它们通常需要有意义的、与数据相关的标签来进行有效的训练。

## Squeeze用于去除张量多余维度

在PyTorch中，`.squeeze()`方法用于去除张量（Tensor）中所有长度为1的维度。这个方法通常用于去除多余的、单一维度，使张量的形状更加紧凑。

例如，假设你有一个形状为`[1, 10, 1]`的张量，这意味着它有一个维度大小为1的批次大小和另一个维度大小为1的特征。使用`.squeeze()`方法后，这个张量的形状将变为`[10]`，因为所有长度为1的维度都被移除了。

在机器学习模型训练的上下文中，`y_batch = y_batch.squeeze()`通常用于处理目标标签（labels）或输出。例如，如果模型的输出层是一个单神经元，那么输出张量的形状可能会是`[batch_size, 1]`，即使批量大小为1，它的形状也可能是`[1, 1]`。在这种情况下，使用`.squeeze()`方法可以将其转换为一维张量，形状为`[batch_size]`或`[]`（如果原始形状是`[1]`）。

这样做的好处是，去除单一维度后，张量的形状通常更易于处理，尤其是在计算损失函数或执行其他操作时，可以避免因形状不匹配而产生的错误。

举个例子：
```python
import torch

# 假设 y_batch 是一个形状为 [batch_size, 1] 的张量
y_batch = torch.tensor([[1], [2], [3]])

# 使用 squeeze() 方法去除长度为1的维度
y_batch = y_batch.squeeze()

# 现在 y_batch 的形状是 [batch_size]
print(y_batch.shape)  # 输出: torch.Size([3])
```

在这个例子中，`y_batch.squeeze()`将形状从`[3, 1]`变为`[3]`。如果原始张量的形状已经是一维的，例如`[3]`，那么`.squeeze()`方法不会改变它的形状。如果需要去除特定位置的单一维度，可以传递一个参数给`.squeeze()`方法，例如`.squeeze(dim)`。

## 验证集与测试集

在机器学习中，是否在每个训练周期（epoch）都重新随机划分验证集，取决于你的具体需求和数据集的大小。这里有两种常见的做法：

1. **固定验证集**：
   - **优点**：固定验证集可以确保每次训练时评估的条件是一致的，这有助于更准确地比较不同训练周期或不同模型的性能。
   - **缺点**：如果数据集不够大，固定验证集可能导致模型对这部分数据过拟合，从而影响模型在新数据上的表现。

2. **每次训练周期随机划分验证集**：
   - **优点**：每次随机划分可以减少模型对特定验证集的过拟合，使模型更加健壮，更能反映模型在新数据上的表现。
   - **缺点**：由于每次的验证集不同，这可能导致性能评估的波动，使得比较不同训练周期或不同模型的性能变得困难。

在实践中，以下是一些常见的策略：

- **对于较小的数据集**：由于数据量有限，每次训练周期随机划分验证集可能更合适，以避免模型对特定数据子集的过拟合。

- **对于较大的数据集**：固定验证集通常是一个好选择，因为它可以提供更稳定的性能评估。此外，大型数据集即使在固定验证集的情况下，也不太可能出现过拟合。

- **交叉验证**：这是一种更高级的策略，通过将数据集分成多个小的验证集（例如k折交叉验证），并在每个训练周期使用不同的验证集，可以更全面地评估模型的性能。

- **分层抽样**：如果数据集中的类别分布不均匀，使用分层抽样来确保每个验证集中各类别的比例与整个数据集相似，这有助于提高模型评估的准确性。

总的来说，选择哪种策略取决于你的具体需求、数据集的大小和分布。在实际操作中，你可以尝试不同的策略，看看哪种方法最适合你的模型和数据。

不，验证集和测试集应该是完全不同的数据集，它们都来源于原始数据集，但在机器学习的过程中扮演不同的角色：

1. **训练集（Training Set）**：用于训练模型的数据集，模型通过这些数据学习到数据的模式和规律。

2. **验证集（Validation Set）**：在模型训练过程中使用，用于调整模型参数（如超参数选择、学习率调整、模型选择等）和防止模型过拟合。验证集应该与训练集完全分离，以确保模型在未见过的数据上的表现。

3. **测试集（Test Set）**：在模型训练完成后使用，用于最终评估模型性能的数据集。测试集应该完全独立于训练和验证过程，用来测试模型对未见过数据的泛化能力。

通常，原始数据集会被分为三个部分：训练集、验证集和测试集。这个划分可以通过以下方式进行：

- **手动划分**：根据数据集的大小和需求，手动决定每个部分的比例。例如，70%的数据用于训练，15%用于验证，15%用于测试。

- **自动划分**：使用数据预处理工具或库（如Scikit-Learn的`train_test_split`函数）自动划分数据集。

- **分层抽样**：如果数据集中的类别分布不均匀，使用分层抽样确保每个部分的类别分布与整个数据集相似。

重要的是，验证集和测试集不应该重叠，以确保模型评估的准确性和公正性。此外，为了避免数据泄露（即训练过程中不小心使用了测试数据），在实际操作中，通常会先划分出测试集，然后再从剩余的数据中划分出训练集和验证集。这样可以确保测试集完全独立于训练和验证过程。

## vstack用法

`vstack` 是 NumPy 库中的一个函数，用于垂直堆叠数组。它的全称是 "vertical stack"，即垂直方向上的堆叠。当你想要将多个数组按行顺序堆叠成一个更大的数组时，可以使用这个函数。

函数的基本用法如下：

```python
import numpy as np

# 创建两个数组
array1 = np.array([1, 2, 3])
array2 = np.array([4, 5, 6])

# 使用 vstack 垂直堆叠这两个数组
stacked_array = np.vstack((array1, array2))

print(stacked_array)
```

输出将会是：

```
[[1 2 3]
 [4 5 6]]
```

在这个例子中，`array1` 和 `array2` 都是一维数组，`vstack` 将它们堆叠成了一个二维数组。注意，`vstack` 要求所有输入数组在除了堆叠的维度之外，其他维度的大小必须相同。

在机器学习或数据处理中，`vstack` 经常用来合并特征矩阵或标签数组，尤其是在处理多个数据批次时。例如，你可能有多个批次的预测结果或真实标签，每个批次都是一个数组，你可以使用 `vstack` 将它们合并成一个大的数组，以便于进行整体的评估或分析。

在你提供的代码片段中，`vstack` 被用来合并模型在测试集上的所有预测结果和真实标签，以便计算整个测试集上的准确率：

```python
predictions, actuals = vstack(predictions), vstack(actuals)
acc = accuracy_score(actuals, predictions)
```

这里，`predictions` 和 `actuals` 都是列表，其中包含了多个批次的预测结果和真实标签。通过 `vstack`，这些列表中的数组被垂直堆叠成两个大的数组，然后使用 `accuracy_score` 函数计算准确率。